{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "![](https://images-na.ssl-images-amazon.com/images/S/sgp-catalog-images/region_US/paramount-01376-Full-Image_GalleryBackground-en-US-1484000188762._RI_SX940_.jpg)\n",
    "\n",
    "\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81740, 9)\n"
     ]
    }
   ],
   "source": [
    "movieData = pd.read_csv('MovieSummaries/movie.metadata.tsv', sep = '\\t')\n",
    "movieData.columns = ['Wikipedia movie ID', 'Freebase movie ID', 'Name', 'Date', 'BoxOffice', 'Runtime', 'Language', 'Country', 'genres']\n",
    "print(movieData.shape)\n",
    "\n",
    "#movieData[:42]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.replace('{}', np.nan)\n",
    "movieData = movieData.replace('[]', np.nan)\n",
    "\n",
    "#movieData[:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8066, 9)\n"
     ]
    }
   ],
   "source": [
    "movieData.dropna(subset=['Language'], how='any', inplace = True )\n",
    "movieData.dropna(subset=['genres'], how='any', inplace = True )\n",
    "movieData.dropna(subset=['BoxOffice'], how='any', inplace = True )\n",
    "print(movieData.shape)\n",
    "movieData = movieData.reset_index(drop=True) \n",
    "\n",
    "#movieData[:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractTags(string):\n",
    "    return re.findall('\"([\\w\\s]+)\"', str(string))\n",
    "\n",
    "movieData[\"genres\"] = movieData[\"genres\"].apply(extractTags)\n",
    "movieData[\"Language\"] = movieData[\"Language\"].apply(extractTags)\n",
    "movieData[\"Country\"] = movieData[\"Country\"].apply(extractTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8066,)\n",
      "8066\n",
      "0                                    [Musical, Comedy]\n",
      "1    [Costume drama, War film, Epic, Period piece, ...\n",
      "Name: genres, dtype: object\n"
     ]
    }
   ],
   "source": [
    "genres_col = movieData[\"genres\"]\n",
    "print(genres_col.shape)\n",
    "print(len(genres_col))\n",
    "print(genres_col.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list = []\n",
    "\n",
    "for element in genres_col:\n",
    "    #print(element)\n",
    "    my_list.extend(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres_dict = {}\n",
    "\n",
    "for item in my_list:\n",
    "    if item not in genres_dict:\n",
    "        genres_dict[item] = 1\n",
    "    else:\n",
    "        genres_dict[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama: 4192\n",
      "Comedy: 3133\n",
      "Romance Film: 1992\n",
      "Thriller: 1927\n",
      "Action: 1710\n",
      "Crime Fiction: 1255\n",
      "Adventure: 1146\n",
      "Indie: 1034\n",
      "Romantic comedy: 883\n",
      "Family Film: 834\n",
      "Horror: 762\n",
      "Romantic drama: 757\n",
      "Fantasy: 695\n",
      "Mystery: 672\n",
      "Period piece: 664\n",
      "Science Fiction: 643\n",
      "Film adaptation: 601\n",
      "World cinema: 566\n",
      "Crime Thriller: 565\n",
      "Musical: 532\n",
      "Teen: 400\n",
      "Psychological thriller: 396\n",
      "War film: 391\n",
      "Coming of age: 348\n",
      "Black comedy: 348\n",
      "Animation: 322\n",
      "Parody: 318\n",
      "Cult: 316\n",
      "Sports: 299\n",
      "Biography: 286\n",
      "LGBT: 277\n",
      "Family Drama: 269\n",
      "Suspense: 268\n",
      "Western: 258\n",
      "Biographical film: 251\n",
      "Buddy film: 221\n",
      "Costume drama: 219\n",
      "Slapstick: 213\n",
      "Satire: 196\n",
      "Slasher: 191\n",
      "Supernatural: 186\n",
      "Ensemble Film: 182\n",
      "Japanese Movies: 181\n",
      "Action Thrillers: 176\n",
      "Documentary: 169\n",
      "Political drama: 165\n",
      "Martial Arts Film: 151\n",
      "Gangster Film: 137\n",
      "History: 133\n",
      "Screwball comedy: 132\n",
      "Music: 132\n",
      "Sex comedy: 121\n",
      "Superhero movie: 116\n",
      "Road movie: 114\n",
      "Comedy film: 114\n",
      "Epic: 111\n",
      "Domestic Comedy: 111\n",
      "Fantasy Comedy: 110\n",
      "Spy: 109\n",
      "Comedy of manners: 103\n",
      "Fantasy Adventure: 101\n",
      "Crime Drama: 99\n",
      "Gay Themed: 98\n",
      "Melodrama: 97\n",
      "Courtroom Drama: 96\n",
      "Chinese Movies: 91\n",
      "Gay: 86\n",
      "Crime Comedy: 86\n",
      "Gay Interest: 85\n",
      "Computer Animation: 84\n",
      "Disaster: 83\n",
      "Action Comedy: 80\n",
      "Political thriller: 79\n",
      "Political cinema: 79\n",
      "Docudrama: 79\n",
      "Musical Drama: 77\n",
      "Film noir: 77\n",
      "Creature Film: 76\n",
      "Comedy of Errors: 74\n",
      "Adventure Comedy: 73\n",
      "Detective: 72\n",
      "Historical fiction: 71\n",
      "Doomsday film: 68\n",
      "New Hollywood: 64\n",
      "Remake: 63\n",
      "Musical comedy: 61\n",
      "Animal Picture: 61\n",
      "Detective fiction: 58\n",
      "Americana: 58\n",
      "Heist: 57\n",
      "Erotic thriller: 55\n",
      "Superhero: 54\n",
      "Horror Comedy: 54\n",
      "Chase Movie: 52\n",
      "Zombie Film: 50\n",
      "Tragedy: 49\n",
      "Marriage Drama: 48\n",
      "Absurdism: 48\n",
      "Time travel: 47\n",
      "Erotica: 47\n",
      "Gross out: 46\n",
      "Stop motion: 43\n",
      "Airplanes and airports: 43\n",
      "Christian film: 42\n",
      "Anime: 42\n",
      "Dystopia: 41\n",
      "Workplace Comedy: 39\n",
      "Childhood Drama: 39\n",
      "Monster movie: 38\n",
      "Natural horror films: 37\n",
      "Media Satire: 37\n",
      "Feminist Film: 37\n",
      "Swashbuckler films: 36\n",
      "Slice of life story: 35\n",
      "Dance: 35\n",
      "Political satire: 33\n",
      "Costume Adventure: 33\n",
      "Christmas movie: 33\n",
      "Silent film: 32\n",
      "Glamorized Spy Film: 31\n",
      "Comedy Thriller: 31\n",
      "Boxing: 30\n",
      "Stoner film: 29\n",
      "Roadshow theatrical release: 29\n",
      "Prison: 29\n",
      "Erotic Drama: 29\n",
      "Combat Films: 28\n",
      "Caper story: 28\n",
      "Alien Film: 28\n",
      "Short Film: 25\n",
      "Holiday Film: 25\n",
      "Animated Musical: 25\n",
      "Revisionist Western: 23\n",
      "Mockumentary: 23\n",
      "Sword and sorcery films: 22\n",
      "Social issues: 22\n",
      "Medical fiction: 22\n",
      "Historical drama: 22\n",
      "Historical Epic: 22\n",
      "Experimental film: 22\n",
      "Romantic fantasy: 21\n",
      "Religious Film: 21\n",
      "Existentialism: 21\n",
      "Escape Film: 21\n",
      "Art film: 21\n",
      "Addiction Drama: 21\n",
      "Tragicomedy: 20\n",
      "Monster: 20\n",
      "Haunted House Film: 20\n",
      "Sword and sorcery: 19\n",
      "Surrealism: 19\n",
      "Steampunk: 19\n",
      "Auto racing: 19\n",
      "Jungle Film: 18\n",
      "Future noir: 18\n",
      "Social problem film: 17\n",
      "Inspirational Drama: 16\n",
      "Fairy tale: 16\n",
      "Animated cartoon: 16\n",
      "Gothic Film: 15\n",
      "Hip hop movies: 14\n",
      "Comedy Western: 13\n",
      "Heavenly Comedy: 12\n",
      "Film: 11\n",
      "Blaxploitation: 11\n",
      "Adult: 11\n",
      "Whodunit: 10\n",
      "Sexploitation: 10\n",
      "Indian Western: 10\n",
      "Epic Western: 10\n",
      "Costume Horror: 10\n",
      "Albino bias: 10\n",
      "Rockumentary: 9\n",
      "Master Criminal Films: 9\n",
      "Hybrid Western: 9\n",
      "Hagiography: 9\n",
      "Fantasy Drama: 9\n",
      "Concert film: 9\n",
      "Wuxia: 8\n",
      "Television movie: 8\n",
      "Pornographic movie: 8\n",
      "Buddy cop: 8\n",
      "Sword and Sandal: 7\n",
      "Reboot: 7\n",
      "Nature: 7\n",
      "British Empire Film: 7\n",
      "Bollywood: 7\n",
      "Biker Film: 7\n",
      "Backstage Musical: 7\n",
      "Travel: 6\n",
      "Splatter film: 6\n",
      "Natural disaster: 6\n",
      "Mythological Fantasy: 6\n",
      "Legal drama: 6\n",
      "Jukebox musical: 6\n",
      "Archives and records: 6\n",
      "Animals: 6\n",
      "Space western: 5\n",
      "Space opera: 5\n",
      "Softcore Porn: 5\n",
      "Prison film: 5\n",
      "Plague: 5\n",
      "Outlaw biker film: 5\n",
      "Gender Issues: 5\n",
      "Extreme Sports: 5\n",
      "Demonic child: 5\n",
      "Courtroom Comedy: 5\n",
      "Cold War: 5\n",
      "Baseball: 5\n",
      "Spaghetti Western: 4\n",
      "School story: 4\n",
      "Punk rock: 4\n",
      "Propaganda film: 4\n",
      "Outlaw: 4\n",
      "Mumblecore: 4\n",
      "Kafkaesque: 4\n",
      "Juvenile Delinquency Film: 4\n",
      "Interpersonal Relationships: 4\n",
      "Environmental Science: 4\n",
      "Crime: 4\n",
      "Business: 4\n",
      "Werewolf fiction: 3\n",
      "Vampire movies: 3\n",
      "Therimin music: 3\n",
      "Tamil cinema: 3\n",
      "Samurai cinema: 3\n",
      "Parkour in popular culture: 3\n",
      "Northern: 3\n",
      "Movies About Gladiators: 3\n",
      "Media Studies: 3\n",
      "Libraries and librarians: 3\n",
      "Humour: 3\n",
      "Gulf War: 3\n",
      "Female buddy film: 3\n",
      "Dogme 95: 3\n",
      "Cyberpunk: 3\n",
      "Comedy horror: 3\n",
      "Anthropology: 3\n",
      "Anthology: 3\n",
      "Alien invasion: 3\n",
      "World History: 2\n",
      "Women in prison films: 2\n",
      "Science fiction Western: 2\n",
      "Revisionist Fairy Tale: 2\n",
      "Live action: 2\n",
      "Journalism: 2\n",
      "Instrumental Music: 2\n",
      "Horse racing: 2\n",
      "Hardcore pornography: 2\n",
      "Filipino Movies: 2\n",
      "Feature film: 2\n",
      "Essay Film: 2\n",
      "Educational: 2\n",
      "Cavalry Film: 2\n",
      "British New Wave: 2\n",
      "Acid western: 2\n",
      "The Netherlands in World War II: 1\n",
      "Supermarionation: 1\n",
      "Star vehicle: 1\n",
      "Psychological horror: 1\n",
      "Private military company: 1\n",
      "Point of view shot: 1\n",
      "Period Horror: 1\n",
      "Ninja movie: 1\n",
      "News: 1\n",
      "Kitchen sink realism: 1\n",
      "Goat gland: 1\n",
      "Gay pornography: 1\n",
      "Filmed Play: 1\n",
      "Fictional film: 1\n",
      "Comdedy: 1\n",
      "Clay animation: 1\n",
      "Camp: 1\n",
      "Breakdance: 1\n",
      "Beach Film: 1\n",
      "Archaeology: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(genres_dict.items(), key=lambda item: (item[1], item[0]), reverse=True):\n",
    "    print (\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movieData.dropna(subset=['BoxOffice'], how='any', inplace = True )\n",
    "print(movieData.shape)\n",
    "movieData = movieData.reset_index(drop=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Genres are: Drama, Comedy, Romance Film, Thriller, Action, Crime Fiction,  Adventure, Indie, Romantic comedy, Family Film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.sort_values('BoxOffice', ascending=False)\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#573, 5773, 6513, 7217, 7349, 7994 ###start here\n",
    "movieData = movieData.drop(movieData.index[[573]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.drop(movieData.index[[5773]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.drop(movieData.index[[6515]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.drop(movieData.index[[7223]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.drop(movieData.index[[7360]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieData = movieData.drop(movieData.index[[8013]])\n",
    "movieData = movieData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8054, 9)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID</th>\n",
       "      <th>Freebase movie ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>16395026</td>\n",
       "      <td>/m/03y062v</td>\n",
       "      <td>The Leading Man</td>\n",
       "      <td>1996</td>\n",
       "      <td>18012.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United Kingdom]</td>\n",
       "      <td>[Romantic comedy, Romance Film, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>21133929</td>\n",
       "      <td>/m/05c1msq</td>\n",
       "      <td>Here and There</td>\n",
       "      <td>2009-03-01</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[English Language, Serbian language]</td>\n",
       "      <td>[United States of America, Serbia, Germany]</td>\n",
       "      <td>[Comedy film, Drama, Indie, World cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>13939322</td>\n",
       "      <td>/m/03cntbz</td>\n",
       "      <td>Cruel World</td>\n",
       "      <td>2005-09-01</td>\n",
       "      <td>17986.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Thriller, Comedy film, Horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>16186514</td>\n",
       "      <td>/m/03wc964</td>\n",
       "      <td>Fanny by Gaslight</td>\n",
       "      <td>1944</td>\n",
       "      <td>17285.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United Kingdom]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>20089961</td>\n",
       "      <td>/m/04yhbxd</td>\n",
       "      <td>Holding Trevor</td>\n",
       "      <td>2007</td>\n",
       "      <td>16814.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[LGBT, Indie, Gay, Gay Interest, Drama, Gay Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>1176717</td>\n",
       "      <td>/m/04dp5j</td>\n",
       "      <td>Gummo</td>\n",
       "      <td>1997-08-29</td>\n",
       "      <td>16799.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Ensemble Film, Indie, Experimental film, Dram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>5468633</td>\n",
       "      <td>/m/0dn731</td>\n",
       "      <td>Day Zero</td>\n",
       "      <td>2007-11-02</td>\n",
       "      <td>16659.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Drama, Political drama, Indie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>9054868</td>\n",
       "      <td>/m/027v_s6</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>2008-04-26</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Road movie, Family Drama, Drama, Indie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>4032727</td>\n",
       "      <td>/m/0bdj76</td>\n",
       "      <td>Molly</td>\n",
       "      <td>1999</td>\n",
       "      <td>15593.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Romantic comedy, Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>2885102</td>\n",
       "      <td>/m/0892mn</td>\n",
       "      <td>Save the Green Planet!</td>\n",
       "      <td>2003</td>\n",
       "      <td>15516.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>[Korean Language]</td>\n",
       "      <td>[South Korea]</td>\n",
       "      <td>[Thriller, Science Fiction, Horror, World cine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8020</th>\n",
       "      <td>6060885</td>\n",
       "      <td>/m/0fn369</td>\n",
       "      <td>Blood Red</td>\n",
       "      <td>1989</td>\n",
       "      <td>15510.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Western, Action, Drama, Romance Film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>16357786</td>\n",
       "      <td>/m/03whq7s</td>\n",
       "      <td>Quiet City</td>\n",
       "      <td>2007-08-31</td>\n",
       "      <td>15425.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Romantic drama, Romance Film, Drama, Indie]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Wikipedia movie ID Freebase movie ID                    Name  \\\n",
       "8010            16395026        /m/03y062v         The Leading Man   \n",
       "8011            21133929        /m/05c1msq          Here and There   \n",
       "8012            13939322        /m/03cntbz             Cruel World   \n",
       "8013            16186514        /m/03wc964       Fanny by Gaslight   \n",
       "8014            20089961        /m/04yhbxd          Holding Trevor   \n",
       "8015             1176717         /m/04dp5j                   Gummo   \n",
       "8016             5468633         /m/0dn731                Day Zero   \n",
       "8017             9054868        /m/027v_s6               Tennessee   \n",
       "8018             4032727         /m/0bdj76                   Molly   \n",
       "8019             2885102         /m/0892mn  Save the Green Planet!   \n",
       "8020             6060885         /m/0fn369               Blood Red   \n",
       "8021            16357786        /m/03whq7s              Quiet City   \n",
       "\n",
       "            Date  BoxOffice  Runtime                              Language  \\\n",
       "8010        1996    18012.0     96.0                    [English Language]   \n",
       "8011  2009-03-01    18000.0     81.0  [English Language, Serbian language]   \n",
       "8012  2005-09-01    17986.0     88.0                    [English Language]   \n",
       "8013        1944    17285.0    107.0                    [English Language]   \n",
       "8014        2007    16814.0     88.0                    [English Language]   \n",
       "8015  1997-08-29    16799.0     95.0                    [English Language]   \n",
       "8016  2007-11-02    16659.0     93.0                    [English Language]   \n",
       "8017  2008-04-26    16100.0     99.0                    [English Language]   \n",
       "8018        1999    15593.0     89.0                    [English Language]   \n",
       "8019        2003    15516.0    118.0                     [Korean Language]   \n",
       "8020        1989    15510.0     91.0                    [English Language]   \n",
       "8021  2007-08-31    15425.0     78.0                    [English Language]   \n",
       "\n",
       "                                          Country  \\\n",
       "8010                             [United Kingdom]   \n",
       "8011  [United States of America, Serbia, Germany]   \n",
       "8012                   [United States of America]   \n",
       "8013                             [United Kingdom]   \n",
       "8014                   [United States of America]   \n",
       "8015                   [United States of America]   \n",
       "8016                   [United States of America]   \n",
       "8017                   [United States of America]   \n",
       "8018                   [United States of America]   \n",
       "8019                                [South Korea]   \n",
       "8020                   [United States of America]   \n",
       "8021                   [United States of America]   \n",
       "\n",
       "                                                 genres  \n",
       "8010            [Romantic comedy, Romance Film, Comedy]  \n",
       "8011          [Comedy film, Drama, Indie, World cinema]  \n",
       "8012                    [Thriller, Comedy film, Horror]  \n",
       "8013                                                 []  \n",
       "8014  [LGBT, Indie, Gay, Gay Interest, Drama, Gay Th...  \n",
       "8015  [Ensemble Film, Indie, Experimental film, Dram...  \n",
       "8016                    [Drama, Political drama, Indie]  \n",
       "8017           [Road movie, Family Drama, Drama, Indie]  \n",
       "8018                           [Romantic comedy, Drama]  \n",
       "8019  [Thriller, Science Fiction, Horror, World cine...  \n",
       "8020             [Western, Action, Drama, Romance Film]  \n",
       "8021       [Romantic drama, Romance Film, Drama, Indie]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieData[8010:8022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_genres = [u'Drama', u'Comedy', u'Romance Film', u'Thriller', u'Action', u'Crime Fiction',\n",
    "u'Adventure', u'Indie', u'Romantic comedy', u'Family Film']\n",
    "\n",
    "top10_movie1 = []\n",
    "genres_list = []\n",
    "\n",
    "#for i in range(1, len(movieData)):\n",
    "for i in range(0, 8054):\n",
    "    movie_id = movieData[\"Wikipedia movie ID\"][i]\n",
    "    movie_genres = movieData[\"genres\"][i]\n",
    "    genre_element = movie_genres[0].split(\", \")\n",
    "    top10_movie_genres = []\n",
    "    for element in movie_genres:\n",
    "        if element in top10_genres:\n",
    "            top10_movie1.append(movie_id)\n",
    "            #top10_movie_genres.append(element)\n",
    "            top10_movie_genres.append(element)\n",
    "    genres_list.append(top10_movie_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieData['genres_list'] = genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID</th>\n",
       "      <th>Freebase movie ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4273140</td>\n",
       "      <td>/m/0bth54</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2.782275e+09</td>\n",
       "      <td>178.0</td>\n",
       "      <td>[English Language, Spanish Language]</td>\n",
       "      <td>[United States of America, United Kingdom]</td>\n",
       "      <td>[Thriller, Science Fiction, Adventure, Compute...</td>\n",
       "      <td>[Thriller, Adventure, Action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52371</td>\n",
       "      <td>/m/0dr_4</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>1997-11-01</td>\n",
       "      <td>2.185372e+09</td>\n",
       "      <td>194.0</td>\n",
       "      <td>[Italian Language, English Language, French La...</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Tragedy, Costume drama, Historical fiction, P...</td>\n",
       "      <td>[Drama, Romance Film]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia movie ID Freebase movie ID     Name        Date     BoxOffice  \\\n",
       "0             4273140         /m/0bth54   Avatar  2009-12-10  2.782275e+09   \n",
       "1               52371          /m/0dr_4  Titanic  1997-11-01  2.185372e+09   \n",
       "\n",
       "   Runtime                                           Language  \\\n",
       "0    178.0               [English Language, Spanish Language]   \n",
       "1    194.0  [Italian Language, English Language, French La...   \n",
       "\n",
       "                                      Country  \\\n",
       "0  [United States of America, United Kingdom]   \n",
       "1                  [United States of America]   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [Thriller, Science Fiction, Adventure, Compute...   \n",
       "1  [Tragedy, Costume drama, Historical fiction, P...   \n",
       "\n",
       "                     genres_list  \n",
       "0  [Thriller, Adventure, Action]  \n",
       "1          [Drama, Romance Film]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieData.head(2)\n",
    "#print(movieData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movieData.to_csv('SJ_HW2_metadata.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "match_film = []\n",
    "for each film(F):\n",
    "    for each genres(G) in this F:\n",
    "        if G is in top10_genres:\n",
    "            match_film.extend(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_movie = movieData['Wikipedia movie ID']\n",
    "top10_movie = list(top10_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we have the top 4621 movies!!! Even thought we only need 1000 movies, but just in case some movies don't have a synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42302, 2)\n"
     ]
    }
   ],
   "source": [
    "#now work on plot summary\n",
    "\n",
    "plot = pd.read_csv(\"MovieSummaries/plot_summaries.txt\", sep='\\t')\n",
    "\n",
    "plot.columns = ['Wikipedia movie ID', 'Synopsis']\n",
    "print(plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot[~plot['Wikipedia movie ID'].isin(top10_movie)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34984, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6714, 12)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.merge(movieData, plot, left_index=True, right_index=True, how='inner')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID_x</th>\n",
       "      <th>Freebase movie ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_list</th>\n",
       "      <th>Wikipedia movie ID_y</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52371</td>\n",
       "      <td>/m/0dr_4</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>1997-11-01</td>\n",
       "      <td>2.185372e+09</td>\n",
       "      <td>194.0</td>\n",
       "      <td>[Italian Language, English Language, French La...</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Tragedy, Costume drama, Historical fiction, P...</td>\n",
       "      <td>[Drama, Romance Film]</td>\n",
       "      <td>20663735</td>\n",
       "      <td>Poovalli Induchoodan  is sentenced for six yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25001260</td>\n",
       "      <td>/m/0872p_c</td>\n",
       "      <td>Transformers: Dark of the Moon</td>\n",
       "      <td>2011-06-23</td>\n",
       "      <td>1.123747e+09</td>\n",
       "      <td>157.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Alien Film, Science Fiction, Action, Adventure]</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>5272176</td>\n",
       "      <td>The president is on his way to give a speech. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1213838</td>\n",
       "      <td>/m/04hwbq</td>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td>1.063172e+09</td>\n",
       "      <td>102.0</td>\n",
       "      <td>[English Language, Spanish Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Adventure, Computer Animation, Animation, Fan...</td>\n",
       "      <td>[Adventure, Comedy, Family Film]</td>\n",
       "      <td>2462689</td>\n",
       "      <td>Infuriated at being told to write one final co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia movie ID_x Freebase movie ID                            Name  \\\n",
       "1                 52371          /m/0dr_4                         Titanic   \n",
       "4              25001260        /m/0872p_c  Transformers: Dark of the Moon   \n",
       "7               1213838         /m/04hwbq                     Toy Story 3   \n",
       "\n",
       "         Date     BoxOffice  Runtime  \\\n",
       "1  1997-11-01  2.185372e+09    194.0   \n",
       "4  2011-06-23  1.123747e+09    157.0   \n",
       "7  2010-06-12  1.063172e+09    102.0   \n",
       "\n",
       "                                            Language  \\\n",
       "1  [Italian Language, English Language, French La...   \n",
       "4                                 [English Language]   \n",
       "7               [English Language, Spanish Language]   \n",
       "\n",
       "                      Country  \\\n",
       "1  [United States of America]   \n",
       "4  [United States of America]   \n",
       "7  [United States of America]   \n",
       "\n",
       "                                              genres  \\\n",
       "1  [Tragedy, Costume drama, Historical fiction, P...   \n",
       "4   [Alien Film, Science Fiction, Action, Adventure]   \n",
       "7  [Adventure, Computer Animation, Animation, Fan...   \n",
       "\n",
       "                        genres_list  Wikipedia movie ID_y  \\\n",
       "1             [Drama, Romance Film]              20663735   \n",
       "4               [Action, Adventure]               5272176   \n",
       "7  [Adventure, Comedy, Family Film]               2462689   \n",
       "\n",
       "                                            Synopsis  \n",
       "1  Poovalli Induchoodan  is sentenced for six yea...  \n",
       "4  The president is on his way to give a speech. ...  \n",
       "7  Infuriated at being told to write one final co...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.sort_values('BoxOffice', ascending=False)\n",
    "data = data[:1000] ###Finally, our dataset!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie ID_x</th>\n",
       "      <th>Freebase movie ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_list</th>\n",
       "      <th>Wikipedia movie ID_y</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52371</td>\n",
       "      <td>/m/0dr_4</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>1997-11-01</td>\n",
       "      <td>2.185372e+09</td>\n",
       "      <td>194.0</td>\n",
       "      <td>[Italian Language, English Language, French La...</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Tragedy, Costume drama, Historical fiction, P...</td>\n",
       "      <td>[Drama, Romance Film]</td>\n",
       "      <td>20663735</td>\n",
       "      <td>Poovalli Induchoodan  is sentenced for six yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25001260</td>\n",
       "      <td>/m/0872p_c</td>\n",
       "      <td>Transformers: Dark of the Moon</td>\n",
       "      <td>2011-06-23</td>\n",
       "      <td>1.123747e+09</td>\n",
       "      <td>157.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Alien Film, Science Fiction, Action, Adventure]</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>5272176</td>\n",
       "      <td>The president is on his way to give a speech. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1213838</td>\n",
       "      <td>/m/04hwbq</td>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td>1.063172e+09</td>\n",
       "      <td>102.0</td>\n",
       "      <td>[English Language, Spanish Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Adventure, Computer Animation, Animation, Fan...</td>\n",
       "      <td>[Adventure, Comedy, Family Film]</td>\n",
       "      <td>2462689</td>\n",
       "      <td>Infuriated at being told to write one final co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24314116</td>\n",
       "      <td>/m/09v8clw</td>\n",
       "      <td>Pirates of the Caribbean: On Stranger Tides</td>\n",
       "      <td>2011-05-07</td>\n",
       "      <td>1.043872e+09</td>\n",
       "      <td>136.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Swashbuckler films, Adventure, Costume Advent...</td>\n",
       "      <td>[Adventure, Action]</td>\n",
       "      <td>20532852</td>\n",
       "      <td>A line of people  drool at the window of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50793</td>\n",
       "      <td>/m/0ddt_</td>\n",
       "      <td>Star Wars Episode I: The Phantom Menace</td>\n",
       "      <td>1999-05-19</td>\n",
       "      <td>1.027045e+09</td>\n",
       "      <td>136.0</td>\n",
       "      <td>[English Language]</td>\n",
       "      <td>[United States of America]</td>\n",
       "      <td>[Science Fiction, Action, Fantasy, Adventure, ...</td>\n",
       "      <td>[Action, Adventure, Family Film]</td>\n",
       "      <td>15401493</td>\n",
       "      <td>Lola  attempts to gain her father's trust fund...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia movie ID_x Freebase movie ID  \\\n",
       "1                 52371          /m/0dr_4   \n",
       "4              25001260        /m/0872p_c   \n",
       "7               1213838         /m/04hwbq   \n",
       "8              24314116        /m/09v8clw   \n",
       "9                 50793          /m/0ddt_   \n",
       "\n",
       "                                          Name        Date     BoxOffice  \\\n",
       "1                                      Titanic  1997-11-01  2.185372e+09   \n",
       "4               Transformers: Dark of the Moon  2011-06-23  1.123747e+09   \n",
       "7                                  Toy Story 3  2010-06-12  1.063172e+09   \n",
       "8  Pirates of the Caribbean: On Stranger Tides  2011-05-07  1.043872e+09   \n",
       "9      Star Wars Episode I: The Phantom Menace  1999-05-19  1.027045e+09   \n",
       "\n",
       "   Runtime                                           Language  \\\n",
       "1    194.0  [Italian Language, English Language, French La...   \n",
       "4    157.0                                 [English Language]   \n",
       "7    102.0               [English Language, Spanish Language]   \n",
       "8    136.0                                 [English Language]   \n",
       "9    136.0                                 [English Language]   \n",
       "\n",
       "                      Country  \\\n",
       "1  [United States of America]   \n",
       "4  [United States of America]   \n",
       "7  [United States of America]   \n",
       "8  [United States of America]   \n",
       "9  [United States of America]   \n",
       "\n",
       "                                              genres  \\\n",
       "1  [Tragedy, Costume drama, Historical fiction, P...   \n",
       "4   [Alien Film, Science Fiction, Action, Adventure]   \n",
       "7  [Adventure, Computer Animation, Animation, Fan...   \n",
       "8  [Swashbuckler films, Adventure, Costume Advent...   \n",
       "9  [Science Fiction, Action, Fantasy, Adventure, ...   \n",
       "\n",
       "                        genres_list  Wikipedia movie ID_y  \\\n",
       "1             [Drama, Romance Film]              20663735   \n",
       "4               [Action, Adventure]               5272176   \n",
       "7  [Adventure, Comedy, Family Film]               2462689   \n",
       "8               [Adventure, Action]              20532852   \n",
       "9  [Action, Adventure, Family Film]              15401493   \n",
       "\n",
       "                                            Synopsis  \n",
       "1  Poovalli Induchoodan  is sentenced for six yea...  \n",
       "4  The president is on his way to give a speech. ...  \n",
       "7  Infuriated at being told to write one final co...  \n",
       "8  A line of people  drool at the window of the s...  \n",
       "9  Lola  attempts to gain her father's trust fund...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)\n",
    "#data.to_csv('SJ_HW2_data.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the data\n",
    "\n",
    "Make a dataset of 70% train and 30% test. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 12) (300, 12)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n",
    "\n",
    "SVM: http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sentences = []\n",
    "for item in data['Synopsis']:\n",
    "   sentences.extend([sent.split() for sent in PunktSentenceTokenizer().tokenize(item)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sentence1 = []\n",
    "for movie in data:\n",
    "    for item in data['Synopsis'][i]:\n",
    "        sentence1.extend([sent.split() for sent in PunktSentenceTokenizer().tokenize(item)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700,) (700,) (300,) (300,)\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = train['Synopsis']\n",
    "y_train = train['genres_list']\n",
    "x_test = test['Synopsis']\n",
    "y_test = test['genres_list']\n",
    "\n",
    "Synopsis = data['Synopsis']\n",
    "genres = data['genres_list']\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "print(Synopsis.shape, genres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = train.apply(lambda train: w2v_tokenize_text(train['Synopsis']), axis=1).values\n",
    "test_tokenized = test.apply(lambda test: w2v_tokenize_text(test['Synopsis']), axis=1).values\n",
    "\n",
    "#overall:\n",
    "tokenized_word = data.apply(lambda data: w2v_tokenize_text(data['Synopsis']), axis=1).values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "x_train_sent = []\n",
    "for item in x_train:\n",
    "   x_train_sent.extend([sent.split() for sent in PunktSentenceTokenizer().tokenize(item)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "x_test_sent = []\n",
    "for item in x_test:\n",
    "   x_test_sent.extend([sent.split() for sent in PunktSentenceTokenizer().tokenize(item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 18:50:09,718 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-03-11 18:50:09,720 : INFO : collecting all words and their counts\n",
      "2018-03-11 18:50:09,722 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-11 18:50:09,777 : INFO : collected 24293 word types from a corpus of 278978 raw words and 1000 sentences\n",
      "2018-03-11 18:50:09,778 : INFO : Loading a fresh vocabulary\n",
      "2018-03-11 18:50:09,810 : INFO : min_count=2 retains 13169 unique words (54% of original 24293, drops 11124)\n",
      "2018-03-11 18:50:09,812 : INFO : min_count=2 leaves 267854 word corpus (96% of original 278978, drops 11124)\n",
      "2018-03-11 18:50:09,859 : INFO : deleting the raw counts dictionary of 24293 items\n",
      "2018-03-11 18:50:09,862 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2018-03-11 18:50:09,864 : INFO : downsampling leaves estimated 204411 word corpus (76.3% of prior 267854)\n",
      "2018-03-11 18:50:09,908 : INFO : estimated required memory for 13169 words and 150 dimensions: 22387300 bytes\n",
      "2018-03-11 18:50:09,911 : INFO : resetting layer weights\n",
      "2018-03-11 18:50:10,058 : INFO : training model with 10 workers on 13169 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-11 18:50:10,161 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:10,174 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:10,176 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:10,177 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:10,179 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:10,180 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:10,182 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:10,186 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:10,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:10,190 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:10,192 : INFO : EPOCH - 1 : training on 278978 raw words (204118 effective words) took 0.1s, 1582980 effective words/s\n",
      "2018-03-11 18:50:10,349 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:10,362 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:10,366 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:10,369 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:10,372 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:10,382 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:10,384 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:10,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:10,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:10,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:10,394 : INFO : EPOCH - 2 : training on 278978 raw words (204308 effective words) took 0.2s, 1031645 effective words/s\n",
      "2018-03-11 18:50:10,563 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:10,581 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:10,584 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:10,586 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:10,588 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:10,594 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:10,595 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:10,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:10,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:10,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:10,610 : INFO : EPOCH - 3 : training on 278978 raw words (204453 effective words) took 0.2s, 967249 effective words/s\n",
      "2018-03-11 18:50:10,768 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:10,788 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:10,791 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:10,793 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:10,795 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:10,797 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:10,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:10,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:10,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:10,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:10,812 : INFO : EPOCH - 4 : training on 278978 raw words (204530 effective words) took 0.2s, 1043798 effective words/s\n",
      "2018-03-11 18:50:10,973 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:10,998 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:11,000 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:11,002 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:11,004 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:11,006 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:11,008 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:11,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:11,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:11,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:11,022 : INFO : EPOCH - 5 : training on 278978 raw words (204471 effective words) took 0.2s, 992154 effective words/s\n",
      "2018-03-11 18:50:11,024 : INFO : training on a 1394890 raw words (1021880 effective words) took 1.0s, 1059088 effective words/s\n",
      "2018-03-11 18:50:11,028 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-03-11 18:50:11,029 : INFO : training model with 10 workers on 13169 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-11 18:50:11,176 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:11,210 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:11,215 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:11,218 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:11,222 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:11,225 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:11,230 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:11,234 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:11,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:11,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:11,239 : INFO : EPOCH - 1 : training on 278978 raw words (204558 effective words) took 0.2s, 995976 effective words/s\n",
      "2018-03-11 18:50:11,405 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:11,420 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:11,424 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:11,427 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 18:50:11,428 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:11,430 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:11,436 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:11,438 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:11,440 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:11,442 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:11,443 : INFO : EPOCH - 2 : training on 278978 raw words (204277 effective words) took 0.2s, 1028445 effective words/s\n",
      "2018-03-11 18:50:11,593 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:11,614 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:11,617 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:11,620 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:11,629 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:11,631 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:11,634 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:11,636 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:11,638 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:11,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:11,642 : INFO : EPOCH - 3 : training on 278978 raw words (204357 effective words) took 0.2s, 1050481 effective words/s\n",
      "2018-03-11 18:50:11,797 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:11,814 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:11,817 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:11,820 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:11,823 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:11,824 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:11,833 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:11,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:11,836 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:11,837 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:11,839 : INFO : EPOCH - 4 : training on 278978 raw words (204340 effective words) took 0.2s, 1060475 effective words/s\n",
      "2018-03-11 18:50:11,973 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:11,994 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,005 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:12,011 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,013 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,014 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,019 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,029 : INFO : EPOCH - 5 : training on 278978 raw words (204191 effective words) took 0.2s, 1100734 effective words/s\n",
      "2018-03-11 18:50:12,172 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:12,183 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,186 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:12,191 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,193 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,196 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,199 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,210 : INFO : EPOCH - 6 : training on 278978 raw words (204176 effective words) took 0.2s, 1157401 effective words/s\n",
      "2018-03-11 18:50:12,346 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:12,366 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,368 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:12,370 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,372 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,373 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,376 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,379 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,387 : INFO : EPOCH - 7 : training on 278978 raw words (204169 effective words) took 0.2s, 1187285 effective words/s\n",
      "2018-03-11 18:50:12,524 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:12,554 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,558 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:12,561 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,563 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,565 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,573 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,579 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,582 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,584 : INFO : EPOCH - 8 : training on 278978 raw words (204359 effective words) took 0.2s, 1058715 effective words/s\n",
      "2018-03-11 18:50:12,735 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:12,750 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,753 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-03-11 18:50:12,758 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,762 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,765 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,767 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,774 : INFO : EPOCH - 9 : training on 278978 raw words (204559 effective words) took 0.2s, 1106202 effective words/s\n",
      "2018-03-11 18:50:12,918 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-03-11 18:50:12,923 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-03-11 18:50:12,945 : INFO : worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 18:50:12,947 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-03-11 18:50:12,949 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-03-11 18:50:12,952 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-03-11 18:50:12,954 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 18:50:12,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 18:50:12,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 18:50:12,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 18:50:12,970 : INFO : EPOCH - 10 : training on 278978 raw words (204435 effective words) took 0.2s, 1070316 effective words/s\n",
      "2018-03-11 18:50:12,972 : INFO : training on a 2789780 raw words (2043421 effective words) took 1.9s, 1052726 effective words/s\n"
     ]
    }
   ],
   "source": [
    "### PUT GENSIM CODE HERE\n",
    "#docement = list(Synopsis)\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec (tokenized_word, size=150, window=10, min_count=2, workers=10)\n",
    "#sentences\n",
    "model.train(tokenized_word,total_examples=len(tokenized_word),epochs=10)\n",
    "#w2v = dict(zip)\n",
    "#w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "#meanembeddingvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "svc = Pipeline([(\"count_vectorizer\", \n",
    "                 CountVectorizer(analyzer=lambda x: x)), \n",
    "                (\"linear svc\", OneVsRestClassifier(SVC(kernel=\"linear\")))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MeanEmbeddingVectorizer object at 0x257fe0e48>\n"
     ]
    }
   ],
   "source": [
    "w2v_mean = MeanEmbeddingVectorizer(w2v)\n",
    "print(w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model      score\n",
      "-------  -------\n",
      "svc       0.0171\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "\n",
    "all_models = [(\"svc\", svc)]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, train_tokenized, y_train_mat, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for every synopsis:\n",
    "    for every word in synopsis[i]:\n",
    "        .append(word_embedding[word])\n",
    "        average(synopsis[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.root.handlers = []  \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(700, 10) (300, 10)\n",
      "(700,) (300,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = data['genres_list']\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_matrix = MultiLabelBinarizer().fit_transform(y)\n",
    "print(y_matrix.shape)\n",
    "\n",
    "y_train_mat, y_test_mat = train_test_split(y_matrix, test_size=0.3, random_state=42)\n",
    "\n",
    "print(y_train_mat.shape, y_test_mat.shape)\n",
    "\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, random_state=42)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['Synopsis']\n",
    "X_test = test['Synopsis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-13 12:30:04,483 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "2018-03-13 12:31:43,769 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n",
      "2018-03-13 12:31:50,577 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train.apply(lambda train: w2v_tokenize_text(train['Synopsis']), axis=1).values\n",
    "test_tokenized = test.apply(lambda test: w2v_tokenize_text(test['Synopsis']), axis=1).values\n",
    "#train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['Synopsis']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 300) (700, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_word_average.shape, y_train_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_word_average[:1], y_train[:1])\n",
    "#data[data['summary]]notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/5a2d9157f9bf1bf908794051597b7851333dcfca/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='sigmoid',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#one vs rest classifier python svm\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "\n",
    "\n",
    "model3b = OneVsRestClassifier(SVC(decision_function_shape='ovo', kernel = 'sigmoid', probability=True))\n",
    "model3b.fit(X_train_word_average, y_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred1 = model3b.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(pred1, y_test_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)\n",
    "\n",
    "Use Tokenizer from Keras (Tokenizer.fit_on_texts on x_train);\n",
    "\n",
    "then Tokenizer.text_to_sequence on x_train; then receive a t.word_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 3000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in train['Synopsis']]\n",
    "#print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "test_encoded = [one_hot(d, vocab_size) for d in test['Synopsis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 1500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 1500\n",
    "test_padded = pad_sequences(test_encoded, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 512)               768512    \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 773,642\n",
      "Trainable params: 773,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_length,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 1500)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 10)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 70 samples\n",
      "Epoch 1/5\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 5.1415 - acc: 0.6798 - val_loss: 5.4190 - val_acc: 0.6629\n",
      "Epoch 2/5\n",
      "630/630 [==============================] - 0s 469us/step - loss: 5.2119 - acc: 0.6757 - val_loss: 5.2358 - val_acc: 0.6743\n",
      "Epoch 3/5\n",
      "630/630 [==============================] - 0s 560us/step - loss: 5.1778 - acc: 0.6776 - val_loss: 5.2816 - val_acc: 0.6714\n",
      "Epoch 4/5\n",
      "630/630 [==============================] - 0s 483us/step - loss: 5.0045 - acc: 0.6884 - val_loss: 5.0984 - val_acc: 0.6829\n",
      "Epoch 5/5\n",
      "630/630 [==============================] - 0s 484us/step - loss: 4.7791 - acc: 0.7027 - val_loss: 5.1900 - val_acc: 0.6771\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model4a = model.fit(padded_docs, y_train_mat, batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.242857\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, y_train_mat, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 90us/step\n",
      "Test score: 5.035177866617839\n",
      "Test accuracy: 0.6860000141461691\n"
     ]
    }
   ],
   "source": [
    "pred4a = model.evaluate(test_padded, y_test_mat,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', pred4a[0])\n",
    "print('Test accuracy:', pred4a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4a = model.predict(test_padded, batch_size = batch_size)\n",
    "#print(pred4a[210:220])\n",
    "#print(y_test_mat[210:220])\n",
    "#print(x_test[12:13])\n",
    "#print(y_test[29:30])\n",
    "#Action, Adventure, Comedy, CrimeFiction, Drama, Family Film, Indie, RomanceFilm, RomanticComedy, Thriller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building another model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_112 (Dense)            (None, 1024)              1537024   \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 10)                10250     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,547,274\n",
      "Trainable params: 1,547,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Building another model...')\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(max_length,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.85))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 70 samples\n",
      "Epoch 1/5\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.5449 - acc: 0.7006 - val_loss: 4.3198 - val_acc: 0.7314\n",
      "Epoch 2/5\n",
      "630/630 [==============================] - 1s 1ms/step - loss: 4.4934 - acc: 0.7048 - val_loss: 4.8984 - val_acc: 0.6943\n",
      "Epoch 3/5\n",
      "630/630 [==============================] - 1s 1ms/step - loss: 4.5201 - acc: 0.7052 - val_loss: 4.5030 - val_acc: 0.7200\n",
      "Epoch 4/5\n",
      "630/630 [==============================] - 1s 1ms/step - loss: 4.5217 - acc: 0.7033 - val_loss: 4.3656 - val_acc: 0.7286\n",
      "Epoch 5/5\n",
      "630/630 [==============================] - 1s 1ms/step - loss: 4.4763 - acc: 0.7038 - val_loss: 4.4114 - val_acc: 0.7257\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model4b = model.fit(padded_docs, y_train_mat, batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.328571\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, y_train_mat, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 140us/step\n",
      "Test score: 4.524834753672282\n",
      "Test accuracy: 0.7186666584014892\n"
     ]
    }
   ],
   "source": [
    "pred4b = model.evaluate(test_padded, y_test_mat,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', pred4b[0])\n",
    "print('Test accuracy:', pred4b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4b = model.predict(test_padded, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Build a model using ONLY word2vec: Accuracy = 14%\n",
    "\n",
    "Not sure what I did wrong for this model, but it could only predict Action, otherwise they are just blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Movie Synopsis</td><td>Actual Genres                   </td><td>Predicted Genres</td></tr>\n",
       "<tr><td>98    It is the late 1970s, and smuggler David Swansey  specialises in importing goods to war-torn Southern Rhodesia, defying international sanctions imposed on the doomed nation. Swansey is eventually contracted by the Ian Smith administration to arrange an illicit purchase of American-made Iroquois helicopters for counter-insurgency operations against black African nationalists. However, word of his plan soon reaches the latter, who apply strong political pressure to kill the deal in its cradle - the aircraft shipment in question is impounded upon reaching neighbouring South-West Africa. Meanwhile, one of the many indigenous guerillas resisting the white supremacist policies of the Rhodesian regime is Gideon Marunga , veteran combatant and reluctant participant in atrocities directed against unarmed civilians by his fellow insurgents. Marunga discovers that Swansey, with the aid of the Rhodesian Security Forces and South African sympathizers, hopes to lead an armed raid on the airfield where the Iroquois are being temporarily held - with the intention of stealing them across the border into Rhodesia. On the day of the assault, Marunga arrives at the airfield and stalls the attacking troopers, while his accomplices succeed in destroying some of the helicopters. In the firefight which ensues he comes face to face with Swansey, and the two men subsequently share a weary moment of reflection on their stalemate. Both abruptly part ways; the smuggler permits his enemy to escape unarmed into the night. As word of the foiled transaction spreads, Swansey finds himself unable to continue conducting business on the global scale and is restricted to Rhodesia, where he faces conscription into active duty with the armed forces. The film's storyline closes as Marunga and Swansey confront each other on the battlefield again - this time through the sights of their rifles.\n",
       "Name: Synopsis, dtype: object               </td><td>Adventure, Romance Film, Action </td><td>Action          </td></tr>\n",
       "<tr><td>1193    Lia  and Tina  are two beautiful girls who meet and realize that they have a lot in common. They are both young, beautiful and pissed off, so they decide to hitchhike their way to Rome to find a commune where they can stay and live the life of free love. . . or so they think. Things don't go as they have planned though, and soon they become entangled with prostitution, the police and an aggressive gang.\n",
       "Name: Synopsis, dtype: object               </td><td>Crime Fiction, Comedy, Adventure</td><td>Action          </td></tr>\n",
       "<tr><td>375    Mostly the same as the original biblical story, but with notable differences such as, once again, the expanded role of Delilah , the introduction of the garrison commander  who is friends with Samson , more focus upon Samson's relationship with his first wife, a different handling of the 30 garments bet, and, perhaps the most crucial alteration is to the climax. In the original story, maintained in the 1949 film and the 1996 TV remake, is that Samson only regains his strength after his hair has grown long again, thus allowing him to tear down the Philistine temple. Here, however, Samson is taken to the Philistine temple just after his hair has been cut short, and he prays to God to restore his immense strength despite his short hair, and God complies, allowing Samson enough strength to tear down the stone pillars, thus destroying the temple. Unlike the 1949 and 1996 adaptations, Delilah survives to mourn Samson alongside his followers.\n",
       "Name: Synopsis, dtype: object               </td><td>Action, Drama, Thriller         </td><td>Action          </td></tr>\n",
       "<tr><td>454    Impecunious bookmaker's clerk Arnold Grierson, seeing a way to easy money, forces his daughter Margaret to marry wealthy but obnoxious songwriter Nevern, ignoring her romance with local newspaper editor Michael Hardwick. Soon after the wedding, Grierson requests the loan of a significant sum of money from Nevern and is furious and humiliated to be flatly turned down. He begins to make elaborate plans to murder Nevern on the assumption that Margaret will then inherit her husband's estate. Meanwhile the desperately unhappy Margaret has rekindled her relationship with Hardwick. Nevern finds them in a caf together and causes a public scene. Margaret determines that her only course of action is to divorce Nevern, a prospect which horrifies her father. Nevern is in the process of composing a new song, and lodges a draft manuscript with his publisher. Making sure he has set up a foolproof alibi, Grierson goes to Nevern's house and kills him as he is finalising his new composition. As he leaves through one door, Hardwick, intending to ask Nevern to divorce Margaret, arrives through another. Hardwick finds the body and alerts the police, who in the circumstances do not believe his story and arrest him on suspicion of murder. The interested parties later gather at Nevern's home to hear the reading of the will. Margaret is declared the sole inheritor of all her husband's money and assets, to the delight of her father. He is so happy that he begins to whistle, and gives himself away because it is Nevern's finished composition, which he could only have heard by being in the house on the night of the murder.\n",
       "Name: Synopsis, dtype: object               </td><td>Romance Film, Family Film       </td><td>                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "table_3a = [[\"Movie Synopsis\", \"Actual Genres\", \"Predicted Genres\"],\n",
    "            [x_test[11:12], \"Adventure, Romance Film, Action\", \"Action\"],\n",
    "            [x_test[27:28], \"Crime Fiction, Comedy, Adventure\", \"Action\"],\n",
    "            [x_test[244:245], \"Action, Drama, Thriller\", \"Action\"],\n",
    "            [x_test[22:23], \"Romance Film, Family Film\", \" \"]\n",
    "    \n",
    "]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table_3a, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3b: Do the same thing but with pretrained embeddings: Accuracy = 23%\n",
    "\n",
    "This model could predict most of Drama and Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Movie Synopsis</td><td>Actual Genres                  </td><td>Predicted Genres       </td></tr>\n",
       "<tr><td>635    {{Plot}} The story opens in 1941 Poland, with Dymitr Mirga, a prominent Gypsy violin player, entertaining a group of Nazis in a restaurant. At first the Nazis enjoy the entertainment and assure the musicians that the ongoing removal of the region's Jews is being conducted for the sake of the Romani. However, Dymitr Mirga soon realizes the truth, and asks the head of the Gypsy community to lead its evacuation into Hungary, which that time has no Nazis. The leader is reluctant to comply, and the community's council eventually forces him to resign, giving his position instead to Dymitr Mirga. The son of the deposed leader has been betrothed to a beautiful Romani named Zoya Natkin; but she now chooses to marry Dymitr Mirga's son, Roman Mirga. On their ensuing journey to Hungary, some of the Gypsies desert and are massacred by the Nazis. Others voluntarily split off, in hopes that in smaller numbers they will appear to be merchants rather than Gypsies. Dymitr Mirga's small company eventually sells their jewels to buy horses from another Romani community - a great sacrifice, but necessary to enable them to move quickly. Many are nevertheless killed by the Nazis. The sympathetic population gives them burials and provides a chance for their comrades to meet and mourn their loss. In time, the resolute Dymitr Mirga reaches Hungary with his much diminished group of followers, including his wife, his son and daughter-in-law Roman and Zoya, Zoya's family and Roman's \"rival,\" the son of the former leader, who has been killed by Nazis. All Dymitr Mirga's efforts go for nought, however, when the Nazis finally invade Hungary in 1944. A Nazi column takes the Romani in cattle trucks to concentration camps, where the infamous Col. Kruger conducts horrifying experiments on prisoners. Before their arrival, Dymitr Mirga's daughter escapes out through the window of one of the cattle trucks. At the camp. Dymitr Mirga is forced to play for the Nazis, whilst his son Roman receives minor privileges because of his skill as a translator. However, when Roman's wife Zoya dies, the young man begins to consider his father's urging that he escape. Roman approaches his friend and former rival, and recognizing that their families are marked for death, the two agree to make an attempt. The attempt succeeds, and they manage to reconnect with Roman's younger sister who escaped from the cattle truck. The film ends with the war over. As three Romani carriages head off into a sunset, carryingwe assumeRoman, his friend and his younger sister, the narrator concludes that the \"Gypsy nation has yet to receive any compensation.\"\n",
       "Name: Synopsis, dtype: object               </td><td>Action, Crime Fiction, Thriller</td><td>Action, Drama, Thriller</td></tr>\n",
       "<tr><td>679    Dholakpur is suddenly attacked by two fire spitting dragon monsters. As they spew fire and create havoc, King Inderaverma places the responsibility of saving his kingdm on the mighty Chhota Bheem`s shoulders. Meanwhile, Bheem and his friends save a mouse`s life, who happens to be a mushik, Lord Ganesh`s companion mouse. But due to some unfortunate events, Mushik is taken away by the dragons. Lord Ganesh comes down on earth to help his companion. He and Bheem pair-up against the dragons to save humanity.http://woobooks.in/chhota-bheem-and-ganesh.html\n",
       "Name: Synopsis, dtype: object               </td><td>Romance Film, Drama, Comedy    </td><td>Romance Film, Drama    </td></tr>\n",
       "<tr><td>1116    The wives of several top doctors feel neglected by their husbands, so they turn to drink, drugs and sex for solace.\n",
       "Name: Synopsis, dtype: object               </td><td>Action, Thriller               </td><td>Action, Comedy, Drama  </td></tr>\n",
       "<tr><td>375    Mostly the same as the original biblical story, but with notable differences such as, once again, the expanded role of Delilah , the introduction of the garrison commander  who is friends with Samson , more focus upon Samson's relationship with his first wife, a different handling of the 30 garments bet, and, perhaps the most crucial alteration is to the climax. In the original story, maintained in the 1949 film and the 1996 TV remake, is that Samson only regains his strength after his hair has grown long again, thus allowing him to tear down the Philistine temple. Here, however, Samson is taken to the Philistine temple just after his hair has been cut short, and he prays to God to restore his immense strength despite his short hair, and God complies, allowing Samson enough strength to tear down the stone pillars, thus destroying the temple. Unlike the 1949 and 1996 adaptations, Delilah survives to mourn Samson alongside his followers.\n",
       "Name: Synopsis, dtype: object               </td><td>Action, Drama, Thriller        </td><td>Action, Thriller       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Action, Adventure, Comedy, CrimeFiction, Drama, Family Film, Indie, RomanceFilm, RomanticComedy, Thriller\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "table_3b = [[\"Movie Synopsis\", \"Actual Genres\", \"Predicted Genres\"],\n",
    "            [x_test[:1], \"Action, Crime Fiction, Thriller\", \"Action, Drama, Thriller\"],\n",
    "            [x_test[203:204], \"Romance Film, Drama, Comedy\", \"Romance Film, Drama\"],\n",
    "            [x_test[258:259], \"Action, Thriller\", \"Action, Comedy, Drama\"],\n",
    "            [x_test[244:245], \"Action, Drama, Thriller\", \"Action, Thriller\"]\n",
    "    \n",
    "]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table_3b, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model 4a: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras): Accuracy = 69%\n",
    "\n",
    "this model prediction tends to be Romance Film and Action, for the rest of genres, it had a pretty low prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Movie Synopsis</td><td>Actual Genres                   </td><td>Predicted Genres</td></tr>\n",
       "<tr><td>98    It is the late 1970s, and smuggler David Swansey  specialises in importing goods to war-torn Southern Rhodesia, defying international sanctions imposed on the doomed nation. Swansey is eventually contracted by the Ian Smith administration to arrange an illicit purchase of American-made Iroquois helicopters for counter-insurgency operations against black African nationalists. However, word of his plan soon reaches the latter, who apply strong political pressure to kill the deal in its cradle - the aircraft shipment in question is impounded upon reaching neighbouring South-West Africa. Meanwhile, one of the many indigenous guerillas resisting the white supremacist policies of the Rhodesian regime is Gideon Marunga , veteran combatant and reluctant participant in atrocities directed against unarmed civilians by his fellow insurgents. Marunga discovers that Swansey, with the aid of the Rhodesian Security Forces and South African sympathizers, hopes to lead an armed raid on the airfield where the Iroquois are being temporarily held - with the intention of stealing them across the border into Rhodesia. On the day of the assault, Marunga arrives at the airfield and stalls the attacking troopers, while his accomplices succeed in destroying some of the helicopters. In the firefight which ensues he comes face to face with Swansey, and the two men subsequently share a weary moment of reflection on their stalemate. Both abruptly part ways; the smuggler permits his enemy to escape unarmed into the night. As word of the foiled transaction spreads, Swansey finds himself unable to continue conducting business on the global scale and is restricted to Rhodesia, where he faces conscription into active duty with the armed forces. The film's storyline closes as Marunga and Swansey confront each other on the battlefield again - this time through the sights of their rifles.\n",
       "Name: Synopsis, dtype: object               </td><td>Adventure, Romance Film, Action </td><td>Romance Film    </td></tr>\n",
       "<tr><td>454    Impecunious bookmaker's clerk Arnold Grierson, seeing a way to easy money, forces his daughter Margaret to marry wealthy but obnoxious songwriter Nevern, ignoring her romance with local newspaper editor Michael Hardwick. Soon after the wedding, Grierson requests the loan of a significant sum of money from Nevern and is furious and humiliated to be flatly turned down. He begins to make elaborate plans to murder Nevern on the assumption that Margaret will then inherit her husband's estate. Meanwhile the desperately unhappy Margaret has rekindled her relationship with Hardwick. Nevern finds them in a caf together and causes a public scene. Margaret determines that her only course of action is to divorce Nevern, a prospect which horrifies her father. Nevern is in the process of composing a new song, and lodges a draft manuscript with his publisher. Making sure he has set up a foolproof alibi, Grierson goes to Nevern's house and kills him as he is finalising his new composition. As he leaves through one door, Hardwick, intending to ask Nevern to divorce Margaret, arrives through another. Hardwick finds the body and alerts the police, who in the circumstances do not believe his story and arrest him on suspicion of murder. The interested parties later gather at Nevern's home to hear the reading of the will. Margaret is declared the sole inheritor of all her husband's money and assets, to the delight of her father. He is so happy that he begins to whistle, and gives himself away because it is Nevern's finished composition, which he could only have heard by being in the house on the night of the murder.\n",
       "Name: Synopsis, dtype: object               </td><td>Romance Film, Family Film       </td><td>Romance Film    </td></tr>\n",
       "<tr><td>1193    Lia  and Tina  are two beautiful girls who meet and realize that they have a lot in common. They are both young, beautiful and pissed off, so they decide to hitchhike their way to Rome to find a commune where they can stay and live the life of free love. . . or so they think. Things don't go as they have planned though, and soon they become entangled with prostitution, the police and an aggressive gang.\n",
       "Name: Synopsis, dtype: object               </td><td>Crime Fiction, Comedy, Adventure</td><td>Action          </td></tr>\n",
       "<tr><td>375    Mostly the same as the original biblical story, but with notable differences such as, once again, the expanded role of Delilah , the introduction of the garrison commander  who is friends with Samson , more focus upon Samson's relationship with his first wife, a different handling of the 30 garments bet, and, perhaps the most crucial alteration is to the climax. In the original story, maintained in the 1949 film and the 1996 TV remake, is that Samson only regains his strength after his hair has grown long again, thus allowing him to tear down the Philistine temple. Here, however, Samson is taken to the Philistine temple just after his hair has been cut short, and he prays to God to restore his immense strength despite his short hair, and God complies, allowing Samson enough strength to tear down the stone pillars, thus destroying the temple. Unlike the 1949 and 1996 adaptations, Delilah survives to mourn Samson alongside his followers.\n",
       "Name: Synopsis, dtype: object               </td><td>Action, Drama, Thriller         </td><td>Action, Drama   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "table_4a = [[\"Movie Synopsis\", \"Actual Genres\", \"Predicted Genres\"],\n",
    "            [x_test[11:12], \"Adventure, Romance Film, Action\", \"Romance Film\"],\n",
    "            [x_test[22:23], \"Romance Film, Family Film\", \"Romance Film\"],\n",
    "            [x_test[27:28], \"Crime Fiction, Comedy, Adventure\", \"Action\"],\n",
    "            [x_test[244:245], \"Action, Drama, Thriller\", \"Action, Drama\"]\n",
    "    \n",
    "]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table_4a, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4b: Change the architecture of your model and compare the result: Accuracy = 72%\n",
    "\n",
    "this model prediction tends to be Drama, for the rest of genres, it had a pretty low prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Movie Synopsis</td><td>Actual Genres</td><td>Predicted Genres</td></tr>\n",
       "<tr><td>1111    {{Plot}} Aircraft factory worker Barry Kane  is wrongly accused of starting a fire at a Glendale, California airplane plant during World War II, an act of fifth columnist sabotage that killed his friend Mason. Kane believes that the real culprit is a man named Fry  who had handed him a fire extinguisher filled with gasoline, at the plant when the fire broke out, causing Mason's death. When the investigators find no one named Fry on the list of plant workers, they assume Kane is the real saboteur. They visit the home of Mason's mother, to ask if she knows where Kane is, but he has gone to get her some brandy, in an attempt to ease her suffering from the loss of her son. They come back as he returns, but she tells him to leave, breaking down in tears. Kane and Mason had seen Fry's name on an envelope the saboteur had dropped before the fire, so Kane heads to the address, a ranch in the High Desert, catching a ride from a garrulous truck driver. The ranch owner, Charles Tobin , appears to be a well-respected citizen, playing in the pool with his granddaughter, although it is later revealed that he is secretly in league with the saboteurs. The granddaughter hands some mail to Kane, when Tobin goes indoors to call the sheriff to arrest Kane. He returns to gloat, seeing Kane returning the letters, but Kane escapes on horseback, although he doesn't make it very far. In handcuffs, on the way to town, Kane manages to escape from the police, at a bridge blocked by the same truckdriver's vehicle. Kane escapes by jumping off the bridge, and manages to tumble one of the searching sheriff's officers into the river. The helpful truck driver misdirects the searchers, then watches as Kane climbs out of the river below on the other side of the bridge. Kane takes refuge with a kind blind man whose visiting niece is a billboard model, Patricia \"Pat\" Martin . Although her uncle asks her to take Kane to the local blacksmith shop to have his handcuffs removed, she instead attempts to take him to the police, believing it is the right thing to do. Despite her attempt to control Kane by wrapping his handcuffed arms around the steering wheel, Kane manages to turn the tables and kidnaps Martin, protesting his innocence to her. When she stops the car, and gets out, threatening to stop the first car that comes by, he uses the fan-belt pulley of her car's generator to cut off his handcuffs, causing the car to overheat shortly after. They arrive in the abandoned Soda City and stumble into an abandoned mine building, which turns out to be a staging area for the saboteurs' plan to blow up Boulder Dam. Kane is discovered by the saboteurs, but he manages to conceal Martin, and he convinces them the newspaper and radio accounts are true, that he is, in fact, a saboteur in league with them. After finding their plans to destroy the dam foiled, although the storyline does not explain why, Kane convinces the saboteurs to take him with them to New York City. He learns of their plans to sabotage the launching of a new U.S. Navy ship {{USS}} at the Brooklyn shipyard. Kane's performance has fooled Martin as well; she flees and contacts the authorities, hoping to get to New York in time to foil their plans for the next bit of sabotage. The saboteurs arrive in New York City, only to find the phone at their office disconnected, a sign the police are on to them. They drive to a Cut Rate Drugs drugstore, the site of Hitchcock's cameo appearance, where they walk through to a door, into a back room, then into a kitchen and out, into a ballroom, into the mansion of a New York dowager. When they walk into the library, to meet the dowager and other conspirators, Kane finds the captured Martin, who had gone to the police but was betrayed by a corrupt sheriff, part of the conspiracy. As Kane attempts to signal her that she should escape, Tobin arrives, immediately recognizing Kane and denouncing him as a foe of the conspiracy. He sneers at Kane's patriotism, causing Kane to question why someone who has benefited most from living in a free country would work to bring it down. Tobin ridicules Kane's simple-minded belief in good and evil, and claims he is in it for the \"power\". The saboteurs lock Kane in the cellar and Martin in an office at Rockefeller Center. Martin drops a note from her window, alerting cabbies on the street to \"watch for the flickering lights above\". They notify the FBI who rescue her. Meanwhile, desperate to escape, Kane triggers a fire alarm at the mansion and escapes in the pandemonium. Across the street, watching all the servants fleeing the mansion, Kane asks a man on the street if he knows whose place it is. The man answers that it is the \"Sutton mansion\", home of a well-known philanthropic older woman. Kane races to the shipyard, abandoning his taxi when it becomes stalled in traffic, because time is running out. At the gate to the Navy Yard, he is stopped by the guard, who turns him over to a Sergeant-of-the-Guard. He then eludes the MP Sergeant taking him to his superior. Desperate to warn someone of the impending sabotage, Kane runs into the Yard, then stumbles onto Fry, at the controls inside a fake newsreel truck. They struggle long enough for Kane to prevent Fry from pushing the bomb's detonator. The ship is safely out of the dock before Fry can detonate the bomb. Coming up with a pistol, Fry holds Kane prisoner, and has his accomplice drive them to Rockefeller Center. When they arrive, they find the police and FBI waiting to arrest them. Fry's flight from the officers takes him into a movie theatre, where he shoots a spectator to cause confusion to allow him to escape in the crowd. As he exits, Kane and Martin are exiting the building, Kane in the custody of an FBI agent. Seeing Fry getting into a taxi, Kane tells her to follow the spy wherever he goes. In a taxi herself, she follows Fry to Battery Park, as he smugly notes a capsized ship in the river while traveling by. At the Battery, she sees him get on a boat to Liberty Island. Martin follows him onto the boat, attracting his attention, then sees him walk into the pedestal. She calls the FBI office, then goes into the Statue herself, climbing to the top of the Statue of Liberty, where she strikes up a conversation with Fry, to stall the spy until Kane and the FBI arrive. An aggressive agent in the FBI office insists on taking Kane with him to the island, where Kane escapes his escort, racing into the pedestal. Martin calls down to Kane that Fry is getting away, so Kane, brought along to identify the spy, follows Fry up the narrow tunnel, onto the torch viewing platform. When Kane emerges from the tunnel, he confronts Fry, who backs up against the railing and loses his balance. Fry falls over the torch's railing, but manages to grab hold of the statue's hand. Kane climbs down in an attempt to rescue Fry. The police and FBI agent finally arrive at the torch, looking over the railing. When Fry's grip slips, Kane quickly grabs the sleeve of Fry's jacket. The camera focuses on the stitching of Fry's jacket sleeve as it parts, opening a tear and eventually giving way, as Fry tumbles to his death, with a loud, dramatic cry, leaving Kane holding the empty sleeve. Kane climbs carefully back up the thumb to the torch railing to embrace the waiting Martin.\n",
       "Name: Synopsis, dtype: object               </td><td>Drama        </td><td>Drama           </td></tr>\n",
       "<tr><td>994    The story tells of a North Korean father and husband who decides to illegally cross into China to buy medicine for his pregnant wife who is suffering from tuberculosis. However, once he crosses into China, he realises that it's not as easy as he thought. He finds himself working as an illegal immigrant under the constant threat of being captured by the Chinese authorities and deported back to North Korea. He eventually finds his way to South Korea by entering the embassy in China. Meanwhile, his wife has already died, leaving their son homeless and wandering around trying to find a way back to his father. Scenes switch between those of the father who is outside North Korea trying to find medicine, and those of the son, who ends up homeless and tries to defect also.\n",
       "Name: Synopsis, dtype: object               </td><td>Drama        </td><td>Drama           </td></tr>\n",
       "<tr><td>318    Dil Ne Jise Apna Kahaa begins with Rishabh  and Pari  who are deeply in love. He is a wealthy young man, working in an advertising agency while she is a hardworking, dedicated doctor. They marry and soon Pari is pregnant. Pari has a dream to create a hospital for children. Tragically, she is involved in an accident and dies in hospital. Pari's last wish was to donate her heart to her patient Dhani . Rishabh is devastated and opposes the plan to donate the heart; he goes ahead with Pari's last request: the creation of a children's hospital. Dhani is cured, much to the joy of her family and her grandmother ([[Helen . Rishabh has gone into depression but soon comes across Pari's project to build a hospital for children. He begins to develop the hospital. Soon enough Rishabh and Dhani come across each other, and she feels an instant attraction to him. Rishabh ignores her advances as he is still very much in love with Pari. Rishabh does not know that Pari's heart was given to Dhani but soon he realizes that. When he does, Dhani faints. While she is in the hospital, with doctors struggling to restart her broken heart, he falls in love with her. He tells her that he loves her, and if she loves him, she will pull through. She does, and the two get together.\n",
       "Name: Synopsis, dtype: object               </td><td>Family Film  </td><td>Drama           </td></tr>\n",
       "<tr><td>65    Ralph Bellamy is \"The Healer\"  in this 1930s morally uplifting pot-boiler. Bellamy is a doctor that has come home to a warm springs to try to heal children from the unnamed crippling disease . He runs a destitute camp for these children, assisted by Evelyn  who looks upon the Doc as a great man. Mickey Rooney is Jimmy, a paraplegic kid whom the Doc promises to cure. This little triangle is interrupted by rich girl Joan who cons the good Doc into building a sanitorium for the wealthy with her father's money. Doc is momentarily swayed, but comes to his senses just as a forest fire threatens his original cabins around the warm spring. His treatment of Jimmy pays off as Jimmy rides a bicycle to save the day. Doc realizes that his true love is Evelyn, not the self-interested Joan.\n",
       "Name: Synopsis, dtype: object               </td><td>Drama, Indie </td><td>Drama           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "table_4b = [[\"Movie Synopsis\", \"Actual Genres\", \"Predicted Genres\"], \n",
    "            [x_test[109:110], \"Drama\", \"Drama\"],\n",
    "            [x_test[209:210], \"Drama\", \"Drama\"],\n",
    "            [x_test[132:133], \"Family Film\", \"Drama\"],\n",
    "            [x_test[223:224], \"Drama, Indie\", \"Drama\"]\n",
    "    \n",
    "]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table_4b, tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
