{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangs19/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "data = pd.read_csv(\"./yelp_labelled.txt\", delimiter='\\t', header=None, names=['review','label'], encoding='utf-8')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review  label\n",
       "0  Wow... Loved this place.      1\n",
       "1        Crust is not good.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN starts here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "for sentences, sentiment in zip(data.review, data.label):\n",
    "    sentences_cleaned = [sent.lower() for sent in sentences]\n",
    "    docs.append(sentences_cleaned)\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "len(docs), len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w',\n",
       " 'o',\n",
       " 'w',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 1024 \n",
    "nb_filter = 256\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "n_out = 2\n",
    "batch_size = 80\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 56\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é',\n",
       " 'ê'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize_sentences(data, char_indices):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [char_indices[w] for w in sentences]\n",
    "        x2 = np.eye(len(char_indices))[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "train_data = vectorize_sentences(docs,char_indices)\n",
    "train_data.shape\n",
    "y_train = to_categorical(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 56)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1024, 56)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(1024, 56), filters=256, kernel_size=7, padding=\"valid\")`\n",
      "  if __name__ == '__main__':\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=7, padding=\"valid\")`\n",
      "  del sys.path[0]\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "  \n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=3, padding=\"valid\")`\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=3, padding=\"valid\")`\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=3, padding=\"valid\")`\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=3, padding=\"valid\")`\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "/Users/jiangs19/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ou...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1024, 56)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1018, 256)         100608    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 339, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 333, 256)          459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 111, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 109, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 107, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 105, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 103, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,312,642\n",
      "Trainable params: 11,312,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
    "\n",
    "conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0],\n",
    "                     border_mode='valid', activation='relu',\n",
    "                     input_shape=(maxlen, vocab_size))(inputs)\n",
    "conv = MaxPooling1D(pool_length=3)(conv)\n",
    "\n",
    "conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1],\n",
    "                      border_mode='valid', activation='relu')(conv)\n",
    "conv1 = MaxPooling1D(pool_length=3)(conv1)\n",
    "\n",
    "conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],\n",
    "                      border_mode='valid', activation='relu')(conv1)\n",
    "\n",
    "conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3],\n",
    "                      border_mode='valid', activation='relu')(conv2)\n",
    "\n",
    "conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4],\n",
    "                      border_mode='valid', activation='relu')(conv3)\n",
    "\n",
    "conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5],\n",
    "                      border_mode='valid', activation='relu')(conv4)\n",
    "conv5 = MaxPooling1D(pool_length=3)(conv5)\n",
    "conv5 = Flatten()(conv5)\n",
    "\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
    "\n",
    "pred = Dense(n_out, activation='softmax', name='output')(z)\n",
    "\n",
    "model = Model(input=inputs, output=pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wow', '...', 'Loved', 'this', 'place', '.'],\n",
       " ['Crust', 'is', 'not', 'good', '.'],\n",
       " ['Not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty', '.'],\n",
       " ['Stopped',\n",
       "  'by',\n",
       "  'during',\n",
       "  'the',\n",
       "  'late',\n",
       "  'May',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'off',\n",
       "  'Rick',\n",
       "  'Steve',\n",
       "  'recommendation',\n",
       "  'and',\n",
       "  'loved',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'selection',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'was',\n",
       "  'great',\n",
       "  'and',\n",
       "  'so',\n",
       "  'were',\n",
       "  'the',\n",
       "  'prices',\n",
       "  '.']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./yelp_labelled.txt\",\n",
    "                   delimiter='\\t', header=None, names=['review','label'],encoding='utf-8')\n",
    "# z = list(nlp.pipe(data['review'], n_threads=20, batch_size=20000))\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from spacy.lang.en import English\n",
    "from collections import Counter\n",
    "\n",
    "raw_text = 'Hello, world. Here are two sentences.'\n",
    "nlp = English()\n",
    "\n",
    "def tokenizeSentences(sent):\n",
    "    doc = nlp(sent)\n",
    "    sentences = [sent.string.strip() for sent in doc]\n",
    "    return sentences\n",
    "\n",
    "Xs = []    \n",
    "for texts in data['review']:\n",
    "    Xs.append(tokenizeSentences(texts))\n",
    "\n",
    "Xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(words) for words in Xs)))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def word_freq(Xs, num):\n",
    "    all_words = [words.lower() for sentences in Xs for words in sentences]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
    "    final_vocab = [k for k,v in sorted_vocab if v>num]\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(final_vocab))\n",
    "    return final_vocab, word_idx\n",
    "\n",
    "final_vocab, word_idx = word_freq(Xs,2)\n",
    "vocab_len = len(final_vocab) # Finally we have 598 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 40)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_sentences(data, word_idx, final_vocab, maxlen=40):\n",
    "    X = []\n",
    "    paddingIdx = len(final_vocab)+2\n",
    "    for sentences in data:\n",
    "        x=[]\n",
    "        for word in sentences:\n",
    "            if word in final_vocab:\n",
    "                x.append(word_idx[word])\n",
    "            elif word.lower() in final_vocab:\n",
    "                x.append(word_idx[word.lower()])\n",
    "            else:\n",
    "                x.append(paddingIdx)\n",
    "        X.append(x)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "train_data = vectorize_sentences(Xs, word_idx, final_vocab)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Flatten, Dropout, Dense, Merge\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam, rmsprop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
