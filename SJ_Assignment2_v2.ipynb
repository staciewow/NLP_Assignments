{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tstacey\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\tstacey\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\__main__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "## DATA FORMATTING\n",
    "# Step 1. Reading the data in!\n",
    "# Metadata ==> id, name of movie, gross boxoffice, genre\n",
    "metadata = pd.read_csv('./MovieSummaries/movie.metadata.tsv', sep='\\t', \n",
    "                       header= None, \n",
    "                       usecols = [0,2,4,8],\n",
    "                      names = ['id','name','gross','genre'])\n",
    "\n",
    "# Plot summaries\n",
    "with open('./MovieSummaries/plot_summaries.txt', encoding='utf8') as fp:\n",
    "    plots = fp.readlines()\n",
    "\n",
    "# Step 2. Cleaning the null values from gross box office\n",
    "filter_metadata = metadata[metadata['gross'].notnull()]\n",
    "\n",
    "# Step 3. Reading in the genre column\n",
    "filter_metadata[\"genre\"] =  filter_metadata[\"genre\"].map(lambda d : list(ast.literal_eval(d).values()))\n",
    "\n",
    "# Step 4: Finding the top genres\n",
    "all_genres = list(filter_metadata['genre'])\n",
    "all_genres_flat = [item for sublist in all_genres for item in sublist]\n",
    "genre_counter = Counter(all_genres_flat)\n",
    "top_genres = [x[0] for x in genre_counter.most_common(10)]\n",
    "\n",
    "# Step 5: Filtering on top genres\n",
    "keep_genres = []\n",
    "for item in all_genres:\n",
    "    gens = list(set(item).intersection(set(top_genres)))\n",
    "    if len(gens)>0:\n",
    "        keep_genres.append(gens)\n",
    "    else:\n",
    "        keep_genres.append(np.nan)\n",
    "\n",
    "filter_metadata['genre'] = keep_genres\n",
    "filter_metadata = filter_metadata[filter_metadata['genre'].notnull()]\n",
    "\n",
    "# Step 6: joining plots to metadata!\n",
    "plots = {x.split('\\t')[0]:x.split('\\t')[1] for x in plots}\n",
    "filter_metadata['plots'] = [plots[str(key)] if str(key) in plots else np.nan for key in filter_metadata['id']]\n",
    "filter_metadata = filter_metadata[filter_metadata['plots'].notnull()]\n",
    "\n",
    "# WHEW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(filter_metadata['plots'], \n",
    "                                                    filter_metadata['genre'], \n",
    "                                                    test_size = .30, \n",
    "                                                    random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building word2vec model\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import string\n",
    "\n",
    "sentences = []\n",
    "for item in filter_metadata['plots']:\n",
    "    sentences.extend([[w.translate(str.maketrans('','',string.punctuation)).strip().lower() for w in sent.split()]\\\n",
    "                      for sent in PunktSentenceTokenizer().tokenize(item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tstacey/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec (sentences, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(sentences,total_examples=len(sentences),epochs=10)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tstacey/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lake', 0.6614973545074463),\n",
       " ('river', 0.6171631813049316),\n",
       " ('stone', 0.6153417229652405),\n",
       " ('creek', 0.6111149787902832),\n",
       " ('shack', 0.604758620262146),\n",
       " ('snow', 0.5995670557022095),\n",
       " ('rope', 0.5986873507499695),\n",
       " ('rocks', 0.5980300903320312),\n",
       " ('hole', 0.5960316061973572),\n",
       " ('trees', 0.5954447984695435)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking word2vec model\n",
    "model.similar_by_word('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(stop_words='english' ,analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "\n",
    "w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"svm\", OneVsRestClassifier(SVC(probability = True)))])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(y_train) \n",
    "test_labels = mlb.transform(y_test)\n",
    "\n",
    "w2v_tfidf.fit(X_train, train_labels)\n",
    "predictions = w2v_tfidf.predict(X_test)\n",
    "pred_probs = w2v_tfidf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5614, Recall: 0.2270\n"
     ]
    }
   ],
   "source": [
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}\".format(precision, recall)) \n",
    "    \n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Texas, Dignan  \"rescues\" Anthony  from a voluntary mental hospital, where he has been staying for self-described exhaustion. Dignan has an elaborate escape planned and has developed a 75-year plan that he shows to Anthony. The plan is to pull off several heists and then meet Mr. Henry, a landscaper and part-time criminal known to Dignan. As a practice heist, the two friends break into Anthony's house, stealing specific items from a list. Afterward, critiquing the heist, Dignan reveals that he \n",
      "\n",
      " Movie has been classified as  ('Drama',) and should be  ('Comedy', 'Crime Fiction', 'Indie')\n",
      "\n",
      "\n",
      "On an academic scholarship, Paul Tannek  is a fish out of water kid from the upstate New York who arrives in New York City. In the fall of 1999, attending college at NYU, Paul runs into repeated complications and mishaps, usually brought on by his roommates, three spoiled, obnoxious party animals. When Paul is branded a loser and kicked out by his roommates, he settles in a room at a veterinary clinic. Afterwards, almost by accident, he meets and falls in love with Dora Diamond , a fellow studen \n",
      "\n",
      " Movie has been classified as  ('Drama',) and should be  ('Comedy', 'Romance Film', 'Romantic comedy')\n",
      "\n",
      "\n",
      "Morris Buttermaker  is a washed-up alcoholic minor-league baseball player who was kicked out of professional baseball for attacking an umpire. He works as an exterminator and is a crude womanizer. He coaches the Bears, a children's baseball team with poor playing skills. They play their first game and do not even make an out before he forfeits the game. Amanda Whurlitzer , a skilled pitcher, is the 12-year-old daughter of one of his ex-girlfriends. At his request, she joins the team. Kelly Leak  \n",
      "\n",
      " Movie has been classified as  ('Drama',) and should be  ('Comedy',)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_invert = mlb.inverse_transform(predictions)\n",
    "label_invert = mlb.inverse_transform(test_labels)\n",
    "\n",
    "pred_data = list(zip(X_test, pred_invert , label_invert, test_labels, pred_probs))\n",
    "\n",
    "for input, prediction, label, lab_arr, pred_arr in pred_data[:5]:\n",
    "    if lab_arr[np.argmax(pred_arr)]!=1:\n",
    "        print(input[:500], '\\n\\n Movie has been classified as ', prediction, 'and should be ', label)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (5037,)\n",
      "x_test shape: (2159,)\n",
      "y_train shape: (5037, 10)\n",
      "y_test shape: (2159, 10)\n",
      "Building model...\n",
      "Train on 4533 samples, validate on 504 samples\n",
      "Epoch 1/20\n",
      "4533/4533 [==============================] - 57s 13ms/step - loss: 5.5628 - acc: 0.2027 - val_loss: 5.0765 - val_acc: 0.2421\n",
      "Epoch 2/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 4.9042 - acc: 0.2672 - val_loss: 4.6364 - val_acc: 0.2698\n",
      "Epoch 3/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 4.5865 - acc: 0.3230 - val_loss: 4.5209 - val_acc: 0.3512\n",
      "Epoch 4/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 4.3935 - acc: 0.3865 - val_loss: 4.4607 - val_acc: 0.4683\n",
      "Epoch 5/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 4.2377 - acc: 0.4278 - val_loss: 4.4865 - val_acc: 0.4782\n",
      "Epoch 6/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 4.1149 - acc: 0.4469 - val_loss: 4.5397 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 4.0276 - acc: 0.4670 - val_loss: 4.5447 - val_acc: 0.5139\n",
      "Epoch 8/20\n",
      "4533/4533 [==============================] - 55s 12ms/step - loss: 3.9420 - acc: 0.4946 - val_loss: 4.5945 - val_acc: 0.4940\n",
      "Epoch 9/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.8300 - acc: 0.4836 - val_loss: 4.7857 - val_acc: 0.4802\n",
      "Epoch 10/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.8004 - acc: 0.4913 - val_loss: 4.7511 - val_acc: 0.5198\n",
      "Epoch 11/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.7363 - acc: 0.5096 - val_loss: 4.8629 - val_acc: 0.5119\n",
      "Epoch 12/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.6597 - acc: 0.5120 - val_loss: 4.9110 - val_acc: 0.5238\n",
      "Epoch 13/20\n",
      "4533/4533 [==============================] - 60s 13ms/step - loss: 3.6064 - acc: 0.5186 - val_loss: 4.9657 - val_acc: 0.5198\n",
      "Epoch 14/20\n",
      "4533/4533 [==============================] - 60s 13ms/step - loss: 3.5895 - acc: 0.5114 - val_loss: 5.0384 - val_acc: 0.5079\n",
      "Epoch 15/20\n",
      "4533/4533 [==============================] - 58s 13ms/step - loss: 3.5373 - acc: 0.5301 - val_loss: 5.1370 - val_acc: 0.4742\n",
      "Epoch 16/20\n",
      "4533/4533 [==============================] - 61s 13ms/step - loss: 3.5341 - acc: 0.5182 - val_loss: 5.1469 - val_acc: 0.5079\n",
      "Epoch 17/20\n",
      "4533/4533 [==============================] - 58s 13ms/step - loss: 3.4799 - acc: 0.5352 - val_loss: 5.2609 - val_acc: 0.5139\n",
      "Epoch 18/20\n",
      "4533/4533 [==============================] - 57s 12ms/step - loss: 3.4497 - acc: 0.5301 - val_loss: 5.3884 - val_acc: 0.5119\n",
      "Epoch 19/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.4135 - acc: 0.5325 - val_loss: 5.4982 - val_acc: 0.5020\n",
      "Epoch 20/20\n",
      "4533/4533 [==============================] - 56s 12ms/step - loss: 3.4114 - acc: 0.5303 - val_loss: 5.5719 - val_acc: 0.5079\n",
      "2159/2159 [==============================] - 2s 960us/step\n",
      "Test score: 5.60009924512\n",
      "Test accuracy: 0.429828624184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.utils import multi_gpu_model\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "num_classes = 10\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "x_train = tokenizer.texts_to_matrix(X_train)\n",
    "x_test = tokenizer.texts_to_matrix(X_test)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "# Borrow our binarized labels from the previous model\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length= x_train.shape[1] ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159/2159 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "\n",
      "Drama 0.816118573414\n",
      "Comedy 0.816118573414\n",
      "Romance Film 0.861509958314\n",
      "Thriller 0.72116720704\n",
      "Action 0.839740620658\n",
      "Action/Adventure 0.666512274201\n",
      "Crime Fiction 0.872163038444\n",
      "Adventure 0.754979157017\n",
      "Indie 0.887447892543\n",
      "Romantic comedy 0.788791106994\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "mlb2 = MultiLabelBinarizer(classes=[str(a) for a in list(range(10))])\n",
    "pred_mat = mlb2.fit_transform([str(a) for a in list(preds)]) \n",
    "\n",
    "print('Accuracy: \\n')\n",
    "for i in range(10):\n",
    "    print(top_genres[i], accuracy_score(y_test[:,i], pred_mat[:,i]))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7490, Recall: 0.3028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}\".format(precision, recall)) \n",
    "    \n",
    "evaluate(test_labels, pred_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
